/**
 * Audio Analysis Custom Hook
 *
 * éŸ³å£°è§£ææ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
 * - AnalyserBridge ã®ç®¡ç†
 * - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰¹å¾´é‡æŠ½å‡º
 * - Intent åˆ†é¡
 * - è§£æçµæœã®æ°¸ç¶šåŒ–
 */

import { useState, useRef, useCallback } from 'react'
import { createAnalyser } from '@/lib/audio/analyserBridge'
import { FeatureAggregator, extractFeatures } from '@/lib/audio/featureExtractor'
import { IntentClassifier } from '@/lib/audio/intentClassifier'
import type { AnalyserBridge, IntentResult, GrainTimeline } from '@/types/audio'

export interface AnalysisResult {
  intentResult: IntentResult | null
  pandaSound: string
  translation: string
  grainTimeline: GrainTimeline[]
}

export interface UseAudioAnalysisConfig {
  audioContext: AudioContext | null
  enabled: boolean
}

export interface UseAudioAnalysisReturn {
  // State
  analyserBridge: AnalyserBridge | null
  isAnalyzing: boolean
  latestAnalysisResult: AnalysisResult | null
  currentIntentResult: IntentResult | null
  currentPandaSound: string
  currentTranslation: string
  currentGrainTimeline: GrainTimeline[]

  // Actions
  initializeAnalyser: () => Promise<AnalyserBridge | null>
  clearCurrentResults: () => void
  startAnalysis: () => void
  stopAnalysisAndProcess: (grainTimeline: GrainTimeline[]) => AnalysisResult | null
  createSafeAnalysisResult: (type?: 'basic' | 'fallback') => AnalysisResult
  setIsAnalyzing: (value: boolean) => void
}

export function useAudioAnalysis(config: UseAudioAnalysisConfig): UseAudioAnalysisReturn {
  const { audioContext, enabled } = config

  // State
  const [analyserBridge, setAnalyserBridge] = useState<AnalyserBridge | null>(null)
  const [isAnalyzing, setIsAnalyzing] = useState(false)
  const [latestAnalysisResult, setLatestAnalysisResult] = useState<AnalysisResult | null>(null)

  // ä¸€æ™‚çš„ãªè§£æçµæœï¼ˆéŸ³å£°å†ç”Ÿä¸­ã®ã¿æœ‰åŠ¹ï¼‰
  const [currentIntentResult, setCurrentIntentResult] = useState<IntentResult | null>(null)
  const [currentPandaSound, setCurrentPandaSound] = useState('')
  const [currentTranslation, setCurrentTranslation] = useState('')
  const [currentGrainTimeline, setCurrentGrainTimeline] = useState<GrainTimeline[]>([])

  // Refs
  const featureAggregatorRef = useRef<FeatureAggregator>(new FeatureAggregator())
  const intentClassifierRef = useRef<IntentClassifier>(new IntentClassifier())
  const analysisIntervalRef = useRef<NodeJS.Timeout | null>(null)

  // AnalyserBridge åˆæœŸåŒ–
  const initializeAnalyser = useCallback(async (): Promise<AnalyserBridge | null> => {
    if (!audioContext || !enabled || analyserBridge) {
      return analyserBridge
    }

    try {
      console.log('ğŸ”¬ Creating analyser bridge...')
      const analyser = createAnalyser(audioContext)
      setAnalyserBridge(analyser)
      console.log('âœ… Analyser bridge created successfully')
      return analyser
    } catch (error) {
      console.error('âŒ Failed to create analyser:', error)
      return null
    }
  }, [audioContext, enabled, analyserBridge])

  // ç¾åœ¨ã®è§£æçµæœã‚’ã‚¯ãƒªã‚¢
  const clearCurrentResults = useCallback(() => {
    console.log('ğŸ”„ Clearing previous analysis state...')
    setCurrentIntentResult(null)
    setCurrentPandaSound('')
    setCurrentTranslation('')
    setCurrentGrainTimeline([])
  }, [])

  // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’ç”Ÿæˆ
  const createSafeAnalysisResult = useCallback((type: 'basic' | 'fallback' = 'basic'): AnalysisResult => {
    const features = type === 'fallback' ? {
      rmsAvg: Math.random() * 0.8 + 0.2,
      rmsMax: Math.random() * 1.0 + 0.5,
      centroidAvg: Math.random() * 2000 + 500,
      centroidMax: Math.random() * 3000 + 1000,
      zcrAvg: Math.random() * 0.2 + 0.05,
      sampleCount: 1
    } : {
      rmsAvg: 0.5,
      rmsMax: 0.8,
      centroidAvg: 1000,
      centroidMax: 1500,
      zcrAvg: 0.1,
      sampleCount: 1
    }

    const intentResult = intentClassifierRef.current.classify(features)
    const pandaSound = intentClassifierRef.current.getRandomPandaSound(intentResult.intent)
    const translation = intentClassifierRef.current.getRandomTranslation(intentResult.intent)

    return { intentResult, pandaSound, translation, grainTimeline: [] }
  }, [])

  // è§£æé–‹å§‹
  const startAnalysis = useCallback(() => {
    if (!enabled || !analyserBridge) {
      console.log('âš ï¸ Analysis disabled or analyser not ready')
      return
    }

    console.log('ğŸµ Starting analysis-enabled speech synthesis with analyser:', !!analyserBridge)
    setIsAnalyzing(true)
    featureAggregatorRef.current.clear()

    // 50msæ¯ã«ç‰¹å¾´é‡ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    analysisIntervalRef.current = setInterval(() => {
      if (analyserBridge) {
        const frequencyData = analyserBridge.getFrequencyFrame()
        const timeData = analyserBridge.getTimeFrame()
        const features = extractFeatures(frequencyData, timeData)
        featureAggregatorRef.current.addSample(features)

        // 10ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«ãƒ­ã‚°
        if (featureAggregatorRef.current.getAggregate().sampleCount % 10 === 0) {
          console.log('ğŸ“Š Sampling features:', featureAggregatorRef.current.getAggregate().sampleCount)
        }
      }
    }, 50) // 20Hz ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
  }, [enabled, analyserBridge])

  // è§£æåœæ­¢ & çµæœç”Ÿæˆ
  const stopAnalysisAndProcess = useCallback((grainTimeline: GrainTimeline[]): AnalysisResult | null => {
    if (!enabled) {
      return null
    }

    console.log('ğŸ” Processing analysis results...', {
      hasInterval: !!analysisIntervalRef.current,
      isEnabled: enabled
    })

    // ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åœæ­¢
    if (analysisIntervalRef.current) {
      clearInterval(analysisIntervalRef.current)
      analysisIntervalRef.current = null
    }

    // ç‰¹å¾´é‡é›†è¨ˆã¨åˆ†é¡
    const aggregate = featureAggregatorRef.current.getAggregate()
    console.log('ğŸ“Š Feature aggregate:', aggregate)

    let result: AnalysisResult

    if (aggregate.sampleCount > 0) {
      const intentResult = intentClassifierRef.current.classify(aggregate)
      const pandaSound = intentClassifierRef.current.getRandomPandaSound(intentResult.intent)
      const translation = intentClassifierRef.current.getRandomTranslation(intentResult.intent)

      console.log('ğŸ¯ Classification result:', { intent: intentResult.intent, confidence: intentResult.confidence })
      console.log('ğŸ¼ Panda sound:', pandaSound)
      console.log('ğŸ—£ï¸ Translation:', translation)

      result = { intentResult, pandaSound, translation, grainTimeline }

      // ç¾åœ¨ã®è§£æçµæœã‚’è¨­å®š
      setCurrentIntentResult(intentResult)
      setCurrentPandaSound(pandaSound)
      setCurrentTranslation(translation)
      setCurrentGrainTimeline(grainTimeline)
    } else {
      console.warn('âš ï¸ No samples collected for analysis, generating fallback results')
      result = createSafeAnalysisResult('fallback')

      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’è¨­å®š
      setCurrentIntentResult(result.intentResult)
      setCurrentPandaSound(result.pandaSound)
      setCurrentTranslation(result.translation)
      setCurrentGrainTimeline(grainTimeline)
    }

    // è§£æçµæœã‚’æ°¸ç¶šåŒ–
    setLatestAnalysisResult(result)
    console.log('âœ… Analysis results set successfully')

    return result
  }, [enabled, createSafeAnalysisResult])

  return {
    // State
    analyserBridge,
    isAnalyzing,
    latestAnalysisResult,
    currentIntentResult,
    currentPandaSound,
    currentTranslation,
    currentGrainTimeline,

    // Actions
    initializeAnalyser,
    clearCurrentResults,
    startAnalysis,
    stopAnalysisAndProcess,
    createSafeAnalysisResult,
    setIsAnalyzing
  }
}
