/**
 * Audio Analysis Custom Hook
 *
 * éŸ³å£°è§£ææ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
 * - AnalyserBridge ã®ç®¡ç†
 * - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰¹å¾´é‡æŠ½å‡º
 * - Intent åˆ†é¡
 * - è§£æçµæœã®æ°¸ç¶šåŒ–
 */

import { useState, useRef, useCallback } from 'react'
import { createAnalyser } from '@/lib/audio/analyserBridge'
import { FeatureAggregator, extractFeatures } from '@/lib/audio/featureExtractor'
import { IntentClassifier } from '@/lib/audio/intentClassifier'
import type { AnalyserBridge, IntentResult, GrainTimeline } from '@/types/audio'

export interface AnalysisResult {
  intentResult: IntentResult | null
  pandaSound: string
  translation: string
  grainTimeline: GrainTimeline[]
}

export interface UseAudioAnalysisConfig {
  audioContext: AudioContext | null
  enabled: boolean
}

export interface UseAudioAnalysisReturn {
  // State
  analyserBridge: AnalyserBridge | null
  isAnalyzing: boolean
  latestAnalysisResult: AnalysisResult | null

  // Actions
  initializeAnalyser: (ctx?: AudioContext) => Promise<AnalyserBridge | null>
  startAnalysis: (bridge?: AnalyserBridge) => void
  stopAnalysisAndProcess: (grainTimeline: GrainTimeline[]) => AnalysisResult | null
  setIsAnalyzing: (value: boolean) => void
}

export function useAudioAnalysis(config: UseAudioAnalysisConfig): UseAudioAnalysisReturn {
  const { audioContext, enabled } = config

  // State
  const [analyserBridge, setAnalyserBridge] = useState<AnalyserBridge | null>(null)
  const [isAnalyzing, setIsAnalyzing] = useState(false)
  const [latestAnalysisResult, setLatestAnalysisResult] = useState<AnalysisResult | null>(null)

  // Refs
  const featureAggregatorRef = useRef<FeatureAggregator>(new FeatureAggregator())
  const intentClassifierRef = useRef<IntentClassifier>(new IntentClassifier())
  const analysisIntervalRef = useRef<NodeJS.Timeout | null>(null)

  // AnalyserBridge åˆæœŸåŒ–
  const initializeAnalyser = useCallback(async (ctx?: AudioContext): Promise<AnalyserBridge | null> => {
    // æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯è¿”ã™
    if (analyserBridge) {
      return analyserBridge
    }

    if (!enabled) {
      return null
    }

    // å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸcontextã‚’å„ªå…ˆã€ãªã‘ã‚Œã°hookã®audioContextã‚’ä½¿ç”¨
    const contextToUse = ctx || audioContext
    if (!contextToUse) {
      console.warn('âš ï¸ AudioContext not available for analyser initialization')
      return null
    }

    try {
      console.log('ğŸ”¬ Creating analyser bridge...')
      const analyser = createAnalyser(contextToUse)
      setAnalyserBridge(analyser)
      console.log('âœ… Analyser bridge created successfully')
      return analyser
    } catch (error) {
      console.error('âŒ Failed to create analyser:', error)
      return null
    }
  }, [audioContext, enabled, analyserBridge])


  // è§£æé–‹å§‹
  const startAnalysis = useCallback((bridge?: AnalyserBridge) => {
    // å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸbridgeã‚’å„ªå…ˆã€ãªã‘ã‚Œã°stateã®analyserBridgeã‚’ä½¿ç”¨
    const bridgeToUse = bridge || analyserBridge

    if (!enabled || !bridgeToUse) {
      console.log('âš ï¸ Analysis disabled or analyser not ready', { enabled, hasBridge: !!bridgeToUse })
      return
    }

    console.log('ğŸµ Starting analysis-enabled speech synthesis with analyser:', !!bridgeToUse)
    setIsAnalyzing(true)
    featureAggregatorRef.current.clear()

    // æ—¢å­˜ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚’ã‚¯ãƒªã‚¢
    if (analysisIntervalRef.current) {
      clearInterval(analysisIntervalRef.current)
    }

    // 50msæ¯ã«ç‰¹å¾´é‡ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    analysisIntervalRef.current = setInterval(() => {
      if (bridgeToUse) {
        const frequencyData = bridgeToUse.getFrequencyFrame()
        const timeData = bridgeToUse.getTimeFrame()
        const features = extractFeatures(frequencyData, timeData)
        featureAggregatorRef.current.addSample(features)

        const currentCount = featureAggregatorRef.current.getAggregate().sampleCount

        // 10ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«ãƒ­ã‚°
        if (currentCount % 10 === 0) {
          console.log('ğŸ“Š Sampling features:', currentCount)
        }
      }
    }, 50) // 20Hz ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
  }, [enabled, analyserBridge])

  // è§£æåœæ­¢ & çµæœç”Ÿæˆ
  const stopAnalysisAndProcess = useCallback((grainTimeline: GrainTimeline[]): AnalysisResult | null => {
    if (!enabled) {
      return null
    }

    console.log('ğŸ” Processing analysis results...', {
      hasInterval: !!analysisIntervalRef.current,
      isEnabled: enabled
    })

    // ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åœæ­¢
    if (analysisIntervalRef.current) {
      clearInterval(analysisIntervalRef.current)
      analysisIntervalRef.current = null
    }

    // ç‰¹å¾´é‡é›†è¨ˆã¨åˆ†é¡
    const aggregate = featureAggregatorRef.current.getAggregate()
    console.log('ğŸ“Š Feature aggregate:', aggregate)

    let result: AnalysisResult

    if (aggregate.sampleCount > 0) {
      const intentResult = intentClassifierRef.current.classify(aggregate)
      const pandaSound = intentClassifierRef.current.getRandomPandaSound(intentResult.intent)
      const translation = intentClassifierRef.current.getRandomTranslation(intentResult.intent)

      console.log('ğŸ¯ Classification result:', { intent: intentResult.intent, confidence: intentResult.confidence })
      console.log('ğŸ¼ Panda sound:', pandaSound)
      console.log('ğŸ—£ï¸ Translation:', translation)

      result = { intentResult, pandaSound, translation, grainTimeline }
    } else {
      console.warn('âš ï¸ No samples collected for analysis')
      // ã‚µãƒ³ãƒ—ãƒ«ãªã—ã®å ´åˆã¯nullã‚’è¿”ã™
      return null
    }

    // è§£æçµæœã‚’æ°¸ç¶šåŒ–
    setLatestAnalysisResult(result)
    console.log('âœ… Analysis results set successfully')

    return result
  }, [enabled])

  return {
    // State
    analyserBridge,
    isAnalyzing,
    latestAnalysisResult,

    // Actions
    initializeAnalyser,
    startAnalysis,
    stopAnalysisAndProcess,
    setIsAnalyzing
  }
}
